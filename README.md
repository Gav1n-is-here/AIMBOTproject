这个项目基于yokov5官方pytorch源码，在detect.py基础上修改得到detectbyme.Py，运行即为aimbot外挂，默认追踪T阵营的head，按o可以在身体和头锁定之间切换，按P可以切换锁定阵营，即CT和T之间切换。
具体实现方面，对鼠标的控制采用ctypes库，定义在detect.py开始的部分，加载数据修改了yolov5的dataloader，继承了里面的mss截图，在原detect代码的数据预处理和加载模型推理结束后在循环里加上对锚框的检测和发送鼠标位移指令的部分，keyboard库实现非阻塞地接受键入q,o,p的指令，分别对应停止循环，切换锁定位置和切换目标阵营
运行方面直接运行不传参默认代码是截取1080p屏幕中心区域推理，传参的指令格式在代码最上方注释
这是一个基于yolov5的对大作业要求功能的简单实现，依赖只在原版yolov5的requirements.txt里加上了ctypes和keyboard（应该），在完成之后看到有yolov10发布并且检测开销大幅降低，于是转到yolov10制作了更完善的版本。
至于训练的模型，yolov5s预训练模型为基础，dataset分支有数据集，实际上来自kaggle唯一的cs2数据集，本次用到的所有模型也在kaggle上训练完毕，（感谢白嫖gpu，在此基础上训练了300 轮，没有启用数据增强(我觉得旋转操作可能会识别尸体，没有做，关于尸体区分在yolov10有更聪明的解决，yolov5的代码很杂乱，数据增强可以直接该数字指定数据增强的概率，但是我没用，因为数据集有一万七千张，已经很充足，训练出来的f1score也有0.89，于是没有再浪费每周免费30小时gpu时长中的4小时打开数据增强再训练一轮，后续yolov10的模型下降率其实还有，300轮还可以继续下去，但是同样效果也很好，F1score也有0.8，所以出于同样的道理我也没有继续训练下去，而且在v10的代码里对于头太小不好检测我找到了一个很好的方法规避，使得模型精度完全够用)


在8核心的cpu上大概需要80ms进行一次推理，没有gpu使得效果不是很好，周末曾试图去网吧跑一下录制效果视频，但是网吧系统不允许我重启，怎么样都装不上cuda，遂作罢，但代码应当问题不大，望助教老师理解
